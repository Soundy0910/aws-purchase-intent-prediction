{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2853d-4646-4c0a-976e-1c33fdef86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile processing_script.py\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_cutoff_timestamp(file_paths):\n",
    "    print(\"PASS 1: Calculating global time cutoff...\")\n",
    "    dates = []\n",
    "    for i, path in enumerate(file_paths):\n",
    "        try:\n",
    "            df_chunk = pd.read_parquet(path)\n",
    "            if 'session_start' not in df_chunk.columns:\n",
    "                continue\n",
    "            dates.append(pd.to_datetime(df_chunk['session_start']))\n",
    "            del df_chunk\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {path}: {e}\")\n",
    "\n",
    "    if not dates:\n",
    "        raise ValueError(\"No valid data found.\")\n",
    "\n",
    "    all_dates = pd.concat(dates, ignore_index=True)\n",
    "    all_dates = all_dates.sort_values()\n",
    "    cutoff_index = int(len(all_dates) * 0.8)\n",
    "    cutoff_date = all_dates.iloc[cutoff_index]\n",
    "    \n",
    "    print(f\"Global Time Cutoff found: {cutoff_date}\")\n",
    "    return cutoff_date\n",
    "\n",
    "def process_file(path, output_dir, cutoff_date):\n",
    "    try:\n",
    "        filename = os.path.basename(path)\n",
    "        df = pd.read_parquet(path)\n",
    "        \n",
    "        # Feature Engineering\n",
    "        if \"session_start\" not in df.columns: return \n",
    "        \n",
    "        df[\"session_start\"] = pd.to_datetime(df[\"session_start\"])\n",
    "        df[\"session_hour\"] = df[\"session_start\"].dt.hour\n",
    "        df[\"session_weekday\"] = df[\"session_start\"].dt.weekday\n",
    "        df[\"is_weekend\"] = df[\"session_weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Ratios\n",
    "        df[\"cart_to_view_ratio\"] = df[\"n_cart\"] / df[\"n_views\"].replace(0, 1)\n",
    "\n",
    "        # --- FINAL CLEAN FEATURE LIST (NO MATH LEAKS) ---\n",
    "        # Removed: n_events (Direct Math Leak)\n",
    "        # Removed: session_duration_seconds (Checkout Time Leak)\n",
    "        # Removed: log_session_duration (Derived from duration)\n",
    "        feature_cols = [\n",
    "            \"n_views\", \n",
    "            \"n_cart\", \n",
    "            \"n_unique_product\", \n",
    "            \"n_unique_category\", \n",
    "            \"session_hour\", \n",
    "            \"session_weekday\", \n",
    "            \"is_weekend\",\n",
    "            \"cart_to_view_ratio\"\n",
    "        ]\n",
    "        \n",
    "        label_col = \"did_purchase\"\n",
    "        \n",
    "        if label_col not in df.columns: return\n",
    "\n",
    "        # Split\n",
    "        train_mask = df[\"session_start\"] < cutoff_date\n",
    "        \n",
    "        # Select Columns\n",
    "        final_cols = feature_cols + [label_col]\n",
    "        df_final = df[final_cols]\n",
    "        \n",
    "        train_chunk = df_final[train_mask]\n",
    "        test_chunk = df_final[~train_mask]\n",
    "\n",
    "        # Write\n",
    "        if not train_chunk.empty:\n",
    "            train_chunk.to_csv(f\"{output_dir}/train/train_{filename}.csv\", index=False, header=False)\n",
    "        if not test_chunk.empty:\n",
    "            test_chunk.to_csv(f\"{output_dir}/test/test_{filename}.csv\", index=False, header=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str, default=\"/opt/ml/processing/input\")\n",
    "    parser.add_argument(\"--output-data\", type=str, default=\"/opt/ml/processing/output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    all_files = glob.glob(os.path.join(args.input_data, \"*\"))\n",
    "    input_files = [f for f in all_files if os.path.isfile(f) and not f.endswith(\".py\")]\n",
    "    \n",
    "    if not input_files:\n",
    "        raise RuntimeError(f\"No files found\")\n",
    "\n",
    "    os.makedirs(os.path.join(args.output_data, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_data, \"test\"), exist_ok=True)\n",
    "\n",
    "    cutoff_date = get_cutoff_timestamp(input_files)\n",
    "    \n",
    "    print(f\"Starting processing...\")\n",
    "    for i, f in enumerate(input_files):\n",
    "        process_file(f, args.output_data, cutoff_date)\n",
    "\n",
    "    print(\"Job Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7930b-94d1-4bbb-83aa-56e8b712c801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "# 1. Configuration\n",
    "region = sagemaker.Session().boto_region_name\n",
    "role = get_execution_role()\n",
    "bucket = \"your-s3-bucket\"  \n",
    "input_prefix = \"features/session_features\" \n",
    "output_prefix = \"processed/data\"           \n",
    "\n",
    "# 2. Define the Processor\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", region=region, version=\"1.2-1\"),\n",
    "    role=role,\n",
    "    instance_count=1,              \n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    base_job_name='clickstream-feature-eng'\n",
    ")\n",
    "\n",
    "# 3. Launch\n",
    "print(f\"Launching Processing Job\")\n",
    "processor.run(\n",
    "    code='processing_script.py',\n",
    "    \n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f\"s3://{bucket}/{input_prefix}\",\n",
    "            destination='/opt/ml/processing/input',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train',\n",
    "            source='/opt/ml/processing/output/train',\n",
    "            destination=f's3://{bucket}/{output_prefix}/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            source='/opt/ml/processing/output/test',\n",
    "            destination=f's3://{bucket}/{output_prefix}/test'\n",
    "        )\n",
    "    ],\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbeac3-c27c-4d7b-a8f3-ab01174b2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "bucket = \"your-s3-bucket\"\n",
    "output_prefix = \"processed/data\"\n",
    "\n",
    "print(\"TRAINING DATA\")\n",
    "train_files = fs.glob(f\"s3://{bucket}/{output_prefix}/train/*.csv\")\n",
    "print(f\"Found {len(train_files)} training parts.\")\n",
    "print(f\"Sample: {train_files[0] if train_files else 'None'}\")\n",
    "\n",
    "print(\"\\nTEST DATA\")\n",
    "test_files = fs.glob(f\"s3://{bucket}/{output_prefix}/test/*.csv\")\n",
    "print(f\"Found {len(test_files)} testing parts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391f07a-b302-4afe-82f3-44b3fbe11ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the file 'train.py' with the Pandas logic\n",
    "script_content = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "def load_dataset(path):\n",
    "    print(f\"Loading CSV files from {path}...\")\n",
    "    files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    if not files:\n",
    "        raise ValueError(f\"No CSV files found in {path}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        # Optimization: Use float32 to save 50% RAM\n",
    "        dfs.append(pd.read_csv(f, header=None, dtype='float32'))\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Label is the LAST column\n",
    "    y = df.iloc[:, -1]\n",
    "    X = df.iloc[:, :-1]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--max_depth', type=int, default=5)\n",
    "    parser.add_argument('--eta', type=float, default=0.2)\n",
    "    parser.add_argument('--num_round', type=int, default=50)\n",
    "    parser.add_argument('--objective', type=str, default='binary:logistic')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading Train Data...\")\n",
    "    X_train, y_train = load_dataset(args.train)\n",
    "    \n",
    "    print(\"Loading Test Data...\")\n",
    "    X_test, y_test = load_dataset(args.test)\n",
    "    \n",
    "    # 2. Convert to DMatrix (Memory Intensive Step)\n",
    "    print(\"Converting to DMatrix...\")\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # Free RAM immediately\n",
    "    del X_train, y_train, X_test, y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # 3. Train\n",
    "    print(\"Starting Training...\")\n",
    "    params = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'objective': args.objective,\n",
    "        'eval_metric': 'auc',\n",
    "        'verbosity': 1\n",
    "    }\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=args.num_round,\n",
    "        evals=[(dtest, \"test\")]\n",
    "    )\n",
    "    \n",
    "    # 4. Save\n",
    "    model_path = os.path.join(args.model_dir, \"xgboost-model\")\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Training Complete. Model saved.\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"train.py\", \"w\") as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"train.py created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72f948-30e1-47ec-b475-9af770c1b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "\n",
    "bucket = \"YOUR_BUCKET_NAME\"  \n",
    "prefix = \"processed/data\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point='train.py',          \n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',   \n",
    "    framework_version='1.5-1',       \n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.2,\n",
    "        'objective': 'binary:logistic',\n",
    "        'num_round': 50,            \n",
    "        'verbosity': 1\n",
    "    },\n",
    "    base_job_name='clickstream-xgb-train'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Launching Training Job using bucket: {bucket}...\")\n",
    "print(f\"Training Data: s3://{bucket}/{prefix}/train/\")\n",
    "print(f\"Testing Data:  s3://{bucket}/{prefix}/test/\")\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': f's3://{bucket}/{prefix}/train/',\n",
    "    'test':  f's3://{bucket}/{prefix}/test/'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95da7f-8be9-4377-a0d0-fadabeed8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "\n",
    "model_s3_uri = xgb_estimator.model_data\n",
    "print(f\"Found model artifact: {model_s3_uri}\")\n",
    "\n",
    "\n",
    "xgb_model = XGBoostModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    entry_point='inference.py',       \n",
    "    framework_version='1.5-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "print(\"Deploying to live endpoint...\")\n",
    "predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "print(f\"Deployment Complete! Endpoint Name: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "254477e0-2d0a-4332-87b3-fa9943d4d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LIVE PREDICTIONS ---\n",
      "Window Shopper: [['0.0052060504']]\n",
      "Serious Buyer:  [['0.60220134']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scenario A: Window Shopper (0 Cart)\n",
    "shopper_no_cart = \"5,0,4,2,14,2,0,0.0\"\n",
    "\n",
    "# Scenario B: Serious Buyer (2 Cart)\n",
    "shopper_with_cart = \"5,2,4,2,14,2,0,0.4\"\n",
    "\n",
    "print(\"\\n--- LIVE PREDICTIONS ---\")\n",
    "print(f\"Window Shopper: {predictor.predict(shopper_no_cart)}\")\n",
    "print(f\"Serious Buyer:  {predictor.predict(shopper_with_cart)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f71d967-01e2-417b-8af8-efa0a4ba41a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2025-12-10-20-44-15-138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2025-12-10-20-44-15-138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Endpoint deleted. No further charges.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Deleting endpoint...\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"✅ Endpoint deleted. No further charges.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
